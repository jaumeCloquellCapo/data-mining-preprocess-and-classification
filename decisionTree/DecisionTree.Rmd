---
title: "DecisiónTree"
output: html_document
---

```{r setup, include=FALSE}
library(tree)
library(rpart)
library(rstudioapi)
library(tree)
library(rJava)
library(partykit)
library(dplyr)
library(party)
library(caret)
library(ipred)
library(RWeka)
library(randomForest)
library(gbm)
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Definimos el path de donde estemos trabajando.
setwd(dirname(getActiveDocumentContext()$path))

# Librerias propias
for (file in list.files("preprocess")) {
  source(paste("preprocess/", file, collapse = NULL, sep = ""))
}
```


## Load Data
```{r}
#Lectura de datos
original.dataset <- readData(files = c("train.csv", "test.csv"))
train <- original.dataset$train
train$C <- as.factor(train$C)
```

# EDA

```{r}
#Dimensiones de la train
summary(train) ## Todas las variables tienen Missing Values y además nos ncontramos cpn outliers ne la mayoría de ellos. x1,x2,x3...
dim(train) #Tenemos 9144 instancias con 50 atributos y una clase binaria.
str(train) #Todas las variables son numéricas

#1. DESBALANCEO
table(train$C)
```

## Missing Values

```{r}
apply(train,2,function(x){
  sum(is.na(x))/length(x)*100
}) %>% sort

## Todas la variables tienen N.A [20%, 54%]
# Variables con más N.A
#X9 -> 54%
#X32 -> 44%
#X40 -> 41%
```
# Tatamiento de los missign Values

```{r}
## If a variable has less than 50% missing values, imputation or prediction might be a viable option.
train.imputed <- subset(train, select = -c(X9))
```

If a variable has less than 50% missing values, imputation or prediction might be a viable option.

Imputation with mean / median / mode
```{r}
library(Hmisc)
train.imputed.mean <- train.imputed
train.imputed.mode <- train.imputed
train.imputed.median <- train.imputed
for (i in length(train.imputed) -1 ) {
  train.imputed.mean[,i] <- impute(train.imputed.mean[,i], mean)
  train.imputed.mode[,i] <- impute(train.imputed.mode[,i], mode)
  train.imputed.median[,i] <- impute(train.imputed.median[,i], median) 
}
```

Prediction
kNN Imputation

```{r}
library(DMwR)
require(robCompositions)
train.imputed.knn <- train.imputed
train.imputed.knn[, !names(train.imputed.knn) %in% "C"] <- knnImputation(train.imputed.knn[, !names(train.imputed.knn) %in% "C"])  # perform knn imputation.
```

Random Forest

```{r}
library(mice)
train.imputed.rf <- train.imputed
miceMod <- mice(train.imputed.rf[, !names(train.imputed.rf) %in% "C"], method="rf")  # perform mice imputation, based on random forests.
train.imputed.rf[, !names(train.imputed.rf) %in% "C"] <- complete(miceMod)  # generate the completed data.
```


# Entrenando nuestro modelo

##Creamos los distintos árboles
```{r}
set.seed(81)
trees <- crear_arbol(train.imputed.rf, "C", withPartitions = FALSE)
```


## Podamos el árbol

```{r}
print_diagnostic(trees)
```

```{r}
plot(trees[[const$treeBagging]][[const$model]])
```
```{r}
trees[[const$treeBagging]][[const$pre]]
```


********************







# Selección de variables, reducción de dimensionalidad

# TDetección outliers


########################
##    TRANSFORMACIÓN  ##
########################

# Normalización ?
# si aplicamos K-means como métodos de discretización, es necesario normalizar y escalar los datos

#Se aplica el centrado y escalado sobre el conjunto de datos, una vez eliminados los valores perdidos
valoresPreprocesados <- caret::preProcess(dataset$test[, -c(dim(train)[2] - 1)],method=c("center","scale"))
train[, -c(dim(train)[2] - 1)] <- predict(valoresPreprocesados,train[, -c(dim(train)[2] - 1)])

  
#####################
## DISCRETIZACIÓN  ##
##################### 
  
#Discretizamos las variables continuas con modelos no supervisados
#train[,1] <- myDiscretization(train, method = 2)

#####################
##     MODELO     ##
##################### 
```{r}

```

