---
title: "DecisiónTree"
output: html_document
---

```{r setup, include=FALSE}
library(tree)
library(rpart)
library(rstudioapi)
library(tree)
library(rJava)
library(partykit)
library(dplyr)
library(party)
library(caret)
library(ipred)
library(RWeka)
library(randomForest)
library(gbm)
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Definimos el path de donde estemos trabajando.
setwd(dirname(getActiveDocumentContext()$path))

# Librerias propias
for (file in list.files("preprocess")) {
  source(paste("preprocess/", file, collapse = NULL, sep = ""))
}
```


## Load Data
```{r}
#Lectura de datos
original.dataset <- readData(files = c("train.csv", "test.csv"))
train <- original.dataset$train
train$C <- as.factor(train$C)
```

# EDA

```{r}
#Dimensiones de la train
summary(train) ## Todas las variables tienen Missing Values y además nos ncontramos cpn outliers ne la mayoría de ellos. x1,x2,x3...
dim(train) #Tenemos 9144 instancias con 50 atributos y una clase binaria.
str(train) #Todas las variables son numéricas

#1. DESBALANCEO
table(train$C)
```

## Missing Values

```{r}
apply(train,2,function(x){
  sum(is.na(x))/length(x)*100
}) %>% sort

## Todas la variables tienen N.A [20%, 54%]
# Variables con más N.A
#X9 -> 54%
#X32 -> 44%
#X40 -> 41%
```
# Tatamiento de los missign Values

opcion 1


```{r}
## If a variable has less than 50% missing values, imputation or prediction might be a viable option.
train.imputed <- subset(train, select = -c(X9))
```

```{r}
## If a variable has more than 50% missing values, drop the variable because you don’t have enough information to ascertain anything useful. 
set.seed(81)
iris.mis <- prodNA(train.imputed, noNA = 0.2)
summary(iris.mis)
iris.imp <- missForest(iris.mis, xtrue = iris, verbose = TRUE)
```

```{r}
library(DMwR)
train.imputed<-knnImputation(train.imputed,k=3,scale = T,meth = "weighAvg")
```



# Entrenando nuestro modelo

##Creamos los distintos árboles (son y sin poda)
```{r}
set.seed(81)
trees <- crear_arbol(train.imputed, "C", withPartitions = TRUE)
```


## Podamos el árbol

```{r}
print_diagnostic(trees)
```

********************







# Selección de variables, reducción de dimensionalidad

# TDetección outliers


########################
##    TRANSFORMACIÓN  ##
########################

# Normalización ?
# si aplicamos K-means como métodos de discretización, es necesario normalizar y escalar los datos

#Se aplica el centrado y escalado sobre el conjunto de datos, una vez eliminados los valores perdidos
valoresPreprocesados <- caret::preProcess(dataset$test[, -c(dim(train)[2] - 1)],method=c("center","scale"))
train[, -c(dim(train)[2] - 1)] <- predict(valoresPreprocesados,train[, -c(dim(train)[2] - 1)])

  
#####################
## DISCRETIZACIÓN  ##
##################### 
  
#Discretizamos las variables continuas con modelos no supervisados
#train[,1] <- myDiscretization(train, method = 2)

#####################
##     MODELO     ##
##################### 
```{r}

```

