---
title: "DecisiónTree"
output: html_document
---

```{r setup, include=FALSE}
library(tree)
library(rpart)
library(rstudioapi)
library(tree)
library(rJava)
library(partykit)
library(dplyr)
library(party)
library(caret)
library(ipred)
library(RWeka)
library(randomForest)
library(gbm)
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Definimos el path de donde estemos trabajando.
setwd(dirname(getActiveDocumentContext()$path))

# Librerias propias
for (file in list.files("preprocess")) {
  source(paste("preprocess/", file, collapse = NULL, sep = ""))
}
```


## Load Data
```{r}
#Lectura de datos
original.dataset <- readData()
train <- original.dataset$train
#train$C <- as.factor(train$C)
```

# EDA

```{r}
#Dimensiones de la train
summary(train) ## Todas las variables tienen Missing Values y además nos ncontramos cpn outliers ne la mayoría de ellos. x1,x2,x3...
dim(train) #Tenemos 9144 instancias con 50 atributos y una clase binaria.
str(train) #Todas las variables son numéricas

#1. DESBALANCEO
table(train$C)
```

## Missing Values http://eio.usc.es/pub/mte/descargas/ProyectosFinMaster/Proyecto_940.pdf
## opciones a estuduiar MissingDataGUI(visualización), amelia(imputación multiple), vim(visualizacion), mice (imputación múltiple utilizando Fully Conditionally Specification)

```{r}
colSums(is.na(train))
```

## Clasificación de los datos perdidos

MCAR (Missing Completely At Random): La probabilidad de que una respuesta a una variable sea dato faltante es independiente tanto del valor de esta variable como del valor de otras variables del conjunto de datos.

MAR (Missing At Random): La probabilidad de que una respuesta sea dato faltante es independiente de los valores de la misma variable pero es dependiente de los valores de otras variables del conjunto de datos.

NMAR (Not Missing At Random): La probabilidad de que una respuesta a una variable sea dato faltante es dependiente de los valores de la variable.

```{r}
library(VIM)
library(Amelia)
library(mice)
library(ggplot2)
aggr(train,numbers=T,sortVar=T)
```



# Tatamiento de los missign Values

```{r}
library(missMDA)
nb <- estim_ncpPCA(train,method.cv = "Kfold", verbose = FALSE) # estimate the number of components from incomplete data
#(available methods include GCV to approximate CV)
nb$ncp #2
plot(0:5, nb$criterion, xlab = "nb dim", ylab = "MSEP")
res.comp <- imputePCA(train, ncp = nb$ncp) # iterativePCA algorithm
res.comp$completeObs[1:3,] # the imputed data set

imp <- cbind.data.frame(res.comp$completeObs,train[,C])
res.pca <- PCA(imp, quanti.sup = 1, quali.sup = 12, ncp = nb$ncp, graph=FALSE)
plot(res.pca, hab=12, lab="quali");
```
```{r}
ntrees=100
iris.imp = missForest(train, variablewise=TRUE,
ntree=ntrees)
iris.imp$OOB
```

```{r}
imp.mice <- mice(train, m = 100, defaultMethod = "norm.boot") # the variability of the parameters is obtained 
res.MIPCA <- MIPCA(don, ncp = 2, nboot = 100) 
res.MIPCA
```


```{r}
## If a variable has less than 50% missing values, imputation or prediction might be a viable option.
train.imputed <- subset(train, select = -c(X9))
```

If a variable has less than 50% missing values, imputation or prediction might be a viable option.

Imputation with mean / median / mode

```{r}
library(Hmisc)
train.imputed.mean <- train
train.imputed.mode <- train
train.imputed.median <- train
for (i in length() -1 ) {
  train.imputed.mean[,i] <- impute(train.imputed.mean[,i], mean)
  train.imputed.mode[,i] <- impute(train.imputed.mode[,i], mode)
  train.imputed.median[,i] <- impute(train.imputed.median[,i], median) 
}
```

Prediction


```{r}
imputed_Data <- mice(subset(train, select = -c(C)), m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imputed_Data)
```
```{r}
completeData <- complete(imputed_Data,5)
colSums(is.na(completeData))
```


kNN Imputation

```{r}
library(DMwR)
require(robCompositions)
train.imputed.knn <- train.imputed
train.imputed.knn[, !names(train.imputed.knn) %in% "C"] <- knnImputation(train.imputed.knn[, !names(train.imputed.knn) %in% "C"])  # perform knn imputation.
```

Random Forest

```{r}
library(mice)
train.imputed.rf <- train.imputed
miceMod <- mice(train.imputed.rf[, !names(train.imputed.rf) %in% "C"], method="rf")  # perform mice imputation, based on random forests.
train.imputed.rf[, !names(train.imputed.rf) %in% "C"] <- complete(miceMod)  # generate the completed data.
```


# Entrenando nuestro modelo

##Creamos los distintos árboles
```{r}
set.seed(81)
trees <- crear_arbol(train.imputed.rf, "C", withPartitions = FALSE)
```


## Podamos el árbol

```{r}
print_diagnostic(trees)
```

```{r}
plot(trees[[const$treeBagging]][[const$model]])
```
```{r}
trees[[const$treeBagging]][[const$pre]]
```


********************







# Selección de variables, reducción de dimensionalidad

# TDetección outliers


########################
##    TRANSFORMACIÓN  ##
########################

# Normalización ?
# si aplicamos K-means como métodos de discretización, es necesario normalizar y escalar los datos

#Se aplica el centrado y escalado sobre el conjunto de datos, una vez eliminados los valores perdidos
valoresPreprocesados <- caret::preProcess(dataset$test[, -c(dim(train)[2] - 1)],method=c("center","scale"))
train[, -c(dim(train)[2] - 1)] <- predict(valoresPreprocesados,train[, -c(dim(train)[2] - 1)])

  
#####################
## DISCRETIZACIÓN  ##
##################### 
  
#Discretizamos las variables continuas con modelos no supervisados
#train[,1] <- myDiscretization(train, method = 2)

#####################
##     MODELO     ##
##################### 
```{r}

```

