entrenar_arbol <- function(sets, objetivo, predictores = ".", mi_cp = .01) {
arbol <- list()
arbol[["modelo"]] <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
arbol[["prediccion"]] <- predict(arbol[["modelo"]], sets[["prueba"]], type = "class")
arbol[["referencia"]] <- sets[["prueba"]][[objetivo]]
bestcp <- arbol[["modelo"]]$cptable[which.min(arbol[["modelo"]]$cptable[,"xerror"]),"CP"]
arbol[["modeloPruned"]] <- prune(arbol[["modelo"]], cp= bestcp)
arbol[["prediccion"]] <- predict(arbol[["modeloPruned"]], sets[["prueba"]], type = "class")
arbol[["referencia"]] <- sets[["prueba"]][[objetivo]]
arbol
}
unarbol <- crear_arbol(cleantrain, "C", mi_cp = 0.005)
unarbol[["diagnostico"]]
obtener_diagnostico <- function(arbol, objetivo, mi_cp = 0) {
diagnostico <- list()
diagnostico[["matriz"]] <- confusionMatrix(data = arbol[["prediccion"]],
reference = arbol[["referencia"]])
cp <- with(arbol[["modelo"]], cptable[which.min(cptable[, "xerror"]), "CP"])
cp_original <- mi_cp
podar <- if(cp < mi_cp) "SI" else "NO"
diagnostico[["mincp"]] <- data.frame("CP mínimo" = cp, "CP original" = cp_original, "Podar" = podar)
cpP <- with(arbol[["modeloPruned"]], cptable[which.min(cptable[, "xerror"]), "CP"])
cp_original <- cpP
podar <- if(cp < mi_cp) "SI" else "NO"
diagnostico[["mincp"]] <- data.frame("CP mínimo" = cp, "CP original" = cp_original, "Podar" = podar)
diagnostico
}
crear_arbol <- function(datos, objetivo, predictores = ".", mi_cp = 0.01) {
resultado <- list()
resultado[["sets"]] <- crear_sets(datos)
resultado[["arbol"]] <- entrenar_arbol(resultado[["sets"]], objetivo, predictores, mi_cp)
resultado[["diagnostico"]] <- obtener_diagnostico(resultado[["arbol"]], objetivo, mi_cp)
resultado
}
set.seed(1986)
unarbol <- crear_arbol(cleantrain, "C", mi_cp = 0.005)
unarbol[["diagnostico"]]
test$pred <- predict(hr_model_pruned, test, type = "class")
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
test$pred <- predict(hr_base_model, test, type = "class")
test$pred <- predict(hr_base_model, cleantest, type = "class")
test$pred <- predict(hr_base_model, cleantest, type = "class")
pred <- predict(hr_base_model, cleantest, type = "class")
pred
set.seed(786)
train_rows <- createDataPartition(cleantrain$C, p = 0.7, list = F)
train_data <- des_data[train_rows, ]
test_data <- des_data[-train_rows, ]
train_rows <- createDataPartition(cleantrain$C, p = 0.7, list = F)
train_data <- cleantrain[train_rows, ]
test_data <- cleantrain[-train_rows, ]
str(train_data)
hr_base_model <- rpart(C ~ ., data = train_data, method = "class",
control = rpart.control(cp = 0))
pred <- predict(hr_base_model, test_data, type = "class")
confusionMatrix(pred, test_data$C)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
hr_base_model<- prune(hr_base_model, cp= bestcp)
pred <- predict(hr_base_model, test_data, type = "class")
confusionMatrix(pred, test_data$C)
bestcp
set.seed(786)
train_rows <- createDataPartition(cleantrain$C, p = 0.7, list = F)
train_data <- cleantrain[train_rows, ]
test_data <- cleantrain[-train_rows, ]
hr_base_model <- rpart(C ~ ., data = train_data, method = "class",
control = rpart.control(cp = 0))
pred <- predict(hr_base_model, test_data, type = "class")
confusionMatrix(pred, test_data$C)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
hr_base_model<- prune(hr_base_model, cp= bestcp)
pred <- predict(hr_base_model, test_data, type = "class")
confusionMatrix(pred, test_data$C)
library(DMwR)
regr.eval(test_data$C, pred)
pred <- predict(hr_base_model, test_data, type = "class")
confusionMatrix(pred, test_data$C)
library(DMwR)
regr.eval(test_data$C, pred)
str(train_data)
str(test_data)
accuracy <- round(sum(pred == test_data$C), digits = 4)
print(paste("The model correctly predicted the test outcome ", accuracy*100, "% of the time", sep=""))
accuracy <- round(sum(pred == test_data$C)/length(pred), digits = 4)
print(paste("The model correctly predicted the test outcome ", accuracy*100, "% of the time", sep=""))
hr_base_model <- rpart(C ~ ., data = train_data, method = "class",
control = rpart.control(cp = 0))
pred <- predict(hr_base_model, test_data, type = "class")
confusionMatrix(pred, test_data$C)
accuracy <- round(sum(pred == test_data$C)/length(pred), digits = 4)
print(paste("The model correctly predicted the test outcome ", accuracy*100, "% of the time", sep=""))
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
hr_base_model<- prune(hr_base_model, cp= bestcp)
pred <- predict(hr_base_model, test_data, type = "class")
confusionMatrix(pred, test_data$C)
library(DMwR)
accuracy <- round(sum(pred == test_data$C)/length(pred), digits = 4)
print(paste("The model correctly predicted the test outcome ", accuracy*100, "% of the time", sep=""))
mse=mean((test_data$C-pred)^2)
rsq.rpart(hr_base_model)
evaluation <- function(model, data, atype) {
cat("\nConfusion matrix:\n")
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$Class)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$Class)/length(data$Class)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(hr_base_model, test_data, "class")
evaluation <- function(model, data, atype) {
cat("\nConfusion matrix:\n")
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(hr_base_model, test_data, "class")
evaluation <- function(model, data, atype) {
cat("\nConfusion matrix:\n")
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
cat("\nConfusion matrix pruned:\n")
bestcp <- model$cptable[which.min(model$cptable[,"xerror"]),"CP"]
model<- prune(model, cp= bestcp)
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(hr_base_model, test_data, "class")
evaluation <- function(model, data, atype) {
cat("\nConfusion matrix:\n")
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
cat("\nConfusion matrix pruned:\n")
bestcp <- model$cptable[which.min(model$cptable[,"xerror"]),"CP"]
cat(bestcp)
model<- prune(model, cp= bestcp)
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(hr_base_model, test_data, "class")
evaluation <- function(model, data, atype) {
cat("\nConfusion matrix:\n")
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
cat("\nConfusion matrix pruned:\n")
bestcp <- model$cptable[which.min(model$cptable[,"xerror"]),"CP"]
modelPruned<- prune(model, cp= bestcp)
prediction = predict(modelPruned, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(hr_base_model, test_data, "class")
evaluation <- function(model, data, atype) {
cat("\nConfusion matrix:\n")
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
cat("\nConfusion matrix pruned:\n")
bestcp <- model$cptable[which.min(model$cptable[,"xerror"]),"CP"]
modelPruned<- prune(model, cp= bestcp)
predictionPruned = predict(modelPruned, data, type=atype)
xtab = table(predictionPruned, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(predictionPruned == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(hr_base_model, test_data, "class")
evaluation <- function(model, data, atype) {
cat("\nConfusion matrix:\n")
prediction = predict(model, data, type=atype)
xtab = table(prediction, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
cat("\nConfusion matrix pruned:\n")
bestcp <- model$cptable[which.min(model$cptable[,"xerror"]),"CP"]
modelPruned<- prune(model, cp= bestcp)
predictionPruned = predict(modelPruned, data, type=atype)
xtab = table(predictionPruned, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(predictionPruned == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluationPruned <- function(model, data, atype) {
cat("\nConfusion matrix pruned:\n")
bestcp <- model$cptable[which.min(model$cptable[,"xerror"]),"CP"]
modelPruned<- prune(model, cp= bestcp)
predictionPruned = predict(modelPruned, data, type=atype)
xtab = table(predictionPruned, data$C)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(predictionPruned == data$C)/length(data$C)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(hr_base_model, test_data, "class")
evaluationPruned(hr_base_model, test_data, "class")
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
pred <- predict(hr_base_model, cleantest, type = "class")
pred
par(mfrow=c(1,2))
rsq.rpart(hr_base_model)
treemap(data,index = c("Category"),vSize ="Sales")
summary(hr_base_model)
#Plot Decision Tree
rpart.plot(hr_base_model, type = 0)
plot(hr_base_model)
# Examine the complexity plot
printcp(hr_base_model)
plotcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
hr_model_pruned<- prune(hr_base_model, cp= bestcp)
rpart.plot(hr_model_pruned)
library("rpart.plot", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
rpart.plot(hr_model_pruned)
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
#Plot Decision Tree
rpart.plot(hr_base_model)
library(randomForest)
library(NoiseFiltersR)
library(kknn)
library(tree)
library(rpart)
library(rstudioapi)
library(tree)
library(rJava)
library(partykit)
library(dplyr)
library(party)
library(caret)
library(ipred)
library(RWeka)
library(randomForest)
library(gbm)
# Definimos el path de donde estemos trabajando.
setwd(dirname(getActiveDocumentContext()$path))
# Librerias propias
for (file in list.files("preprocess")) {
source(paste("preprocess/", file, collapse = NULL, sep = ""))
}
out_univ <- function(i, train){
x = train[,i]
cuantiles <- quantile(x, c(0.25,0.75))
iqr = IQR(x)
x[x < cuantiles[1]-5*iqr | x > cuantiles[2]+5*iqr ] <- NA
return(x)
}
filtrar_univ <- function(train){
sapply(1:50, out_univ, train)
}
filtrar_IPC <- function(train){
filtrado <- IPF(C ~ ., train, s = 2)
return(as.data.frame(filtrado$cleanData))
}
outlier_imput <- function(i, test, train){
x <- train[,i]
y <- test[,i]
cuantiles <- quantile(x, c(0.25, 0.75))
iqr <- IQR(x)
y[y < cuantiles[1]-5*iqr | y > cuantiles[2]+5*iqr ] <- NA
formula <- paste("X", i, "~.", sep = "")
modelo <- kknn(formula, train, test)
y[is.na(y)] <- modelo$fitted.values[is.na(y)]
return(y)
}
FS_forest_importance <- function(formula, x, k=5, imp = 1){
weights <- FSelector::random.forest.importance(formula,x, importance.type = imp)
subset <- FSelector::cutoff.k(weights,k)
Y <- x[,51]
return(cbind(x[,subset], Y))
}
limpieza_total_test <- function(train, test, iter = 1){
for(i in 1:iter){
test <- as.data.frame(sapply(1:50, outlier_imput, test, train))
colnames(test) <- paste("X",1:50, sep = "")
}
return(test)
}
limpieza_total_train <- function(train, iter = 1){
train <- rfImpute(C ~ ., train, iter = 3)
for(i in 1:iter){
train[,2:51] <- filtrar_univ(train[,2:51])
train <- rfImpute(C ~., train, iter = 3)
}
train <- filtrar_IPC(train)
return(train)
}
originalCleantrain <- read.csv("datosFiltrados/train", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantrain <- read.csv("datosFiltrados/train.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/test.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
original.dataset <- readData()
dataCleaned <- list()
train <- original.dataset$train
test <- original.dataset$test
train$C <- as.factor(train$C)
str(dataCleaned[["train"]])
train <- original.dataset$train
test <- original.dataset$test
train$C <- as.factor(train$C)
str(original.dataset$train)
dataCleaned[["train"]] <- limpieza_total_train(train, 1)
str(dataCleaned[["train"]])
#n<-ncol(dataCleaned[["train"]])
#input<-dataCleaned[["train"]][ ,-n]
##dataCleaned[["trainTomek"]]<- ubBalance(X= input, Y=dataCleaned[["train"]]$C, type="ubTomek")
dataCleaned[["test"]] <- limpieza_total_test(dataCleaned[["train"]], test)
str(dataCleaned[["train"]])
limpieza_total_train <- function(train, iter = 1){
train <- rfImpute(C ~ ., train, iter = 3)
for(i in 1:iter){
train[,1:50] <- filtrar_univ(train[,1:50])
train <- rfImpute(C ~., train, iter = 3)
}
train <- filtrar_IPC(train)
return(train)
}
original.dataset <- readData()
dataCleaned <- list()
train <- original.dataset$train
test <- original.dataset$test
train$C <- as.factor(train$C)
dataCleaned[["train"]] <- limpieza_total_train(train, 1)
train <- rfImpute(C ~ ., train, iter = 3)
limpieza_total_train <- function(train, iter = 1){
train <- impute_rf(C~., train, iter = 1)
C<-as.factor(train$C)
for(i in 1:iter){
train<-train[,-1]
train[,1:50] <- filtrar_univ(train[,1:50])
train<-cbind(train, C)
train <- impute_rf(C~., train, iter = 1)
}
train <- filtrar_IPC(train)
return(train)
}
original.dataset <- readData()
dataCleaned <- list()
train <- original.dataset$train
test <- original.dataset$test
train$C <- as.factor(train$C)
dataCleaned[["train"]] <- limpieza_total_train(train, 1)
limpieza_total_train <- function(train, iter = 1){
train <- rfImpute(C~., train, iter = 1)
C<-as.factor(train$C)
for(i in 1:iter){
train<-train[,-1]
train[,1:50] <- filtrar_univ(train[,1:50])
train<-cbind(train, C)
train <- rfImpute(C~., train, iter = 1)
}
train <- filtrar_IPC(train)
return(train)
}
dataCleaned[["train"]] <- limpieza_total_train(train, 1)
#n<-ncol(dataCleaned[["train"]])
#input<-dataCleaned[["train"]][ ,-n]
##dataCleaned[["trainTomek"]]<- ubBalance(X= input, Y=dataCleaned[["train"]]$C, type="ubTomek")
dataCleaned[["test"]] <- limpieza_total_test(dataCleaned[["train"]], test)
#n<-ncol(dataCleaned[["train"]])
#input<-dataCleaned[["train"]][ ,-n]
##dataCleaned[["trainTomek"]]<- ubBalance(X= input, Y=dataCleaned[["train"]]$C, type="ubTomek")
dataCleaned[["test"]] <- limpieza_total_test(dataCleaned[["train"]], test)
str(dataCleaned[["train"]])
dataCleaned[["test"]] <- limpieza_total_test(trainLimpioSinEtiqueta, test)
#n<-ncol(dataCleaned[["train"]])
#input<-dataCleaned[["train"]][ ,-n]
##dataCleaned[["trainTomek"]]<- ubBalance(X= input, Y=dataCleaned[["train"]]$C, type="ubTomek")
trainLimpioSinEtiqueta <- dataCleaned[["train"]][,-1]
dataCleaned[["test"]] <- limpieza_total_test(trainLimpioSinEtiqueta, test)
hr_base_model <- rpart(C ~ ., data = dataCleaned[["train"]], method = "class",
control = rpart.control(cp = 0))
pred <- predict(hr_base_model, dataCleaned[["test"]], type = "class")
summary(hr_base_model)
#Plot Decision Tree
rpart.plot(hr_base_model)
# Examine the complexity plot
printcp(hr_base_model)
plotcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
hr_model_pruned<- prune(hr_base_model, cp= bestcp)
rpart.plot(hr_model_pruned)
pred <- predict(hr_model_pruned, dataCleaned[["test"]], type = "class")
