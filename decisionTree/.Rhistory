return(test)
}
limpieza_total_train <- function(train, iter = 1){
train <- rfImpute(C ~ ., train, iter = 3)
for(i in 1:iter){
train[,2:51] <- filtrar_univ(train[,2:51])
train <- rfImpute(C ~., train, iter = 3)
}
train <- filtrar_IPC(train)
return(train)
}
original.dataset <- readData()
dataCleaned <- list()
train <- original.dataset$train
test <- original.dataset$test
train$C <- as.factor(train$C)
dataCleaned[["train"]] <- limpieza_total_train(train, 3)
dataCleaned[["test"]] <- limpieza_total_test(dataCleaned[["train"]], test)
library(randomForest)
library(NoiseFiltersR)
library(kknn)
library(tree)
library(rpart)
library(rstudioapi)
library(tree)
library(rJava)
library(partykit)
library(dplyr)
library(party)
library(caret)
library(ipred)
library(RWeka)
library(randomForest)
library(gbm)
# Definimos el path de donde estemos trabajando.
setwd(dirname(getActiveDocumentContext()$path))
# Librerias propias
for (file in list.files("preprocess")) {
source(paste("preprocess/", file, collapse = NULL, sep = ""))
}
cleantrain <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/cleantrain.csv")
View(cleantrain)
cleantrain <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/cleantrain.csv")
View(cleantrain)
# Librerias propias
for (file in list.files("preprocess")) {
source(paste("preprocess/", file, collapse = NULL, sep = ""))
}
cleantrain <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/cleantrain.csv")
View(cleantrain)
library(randomForest)
library(NoiseFiltersR)
library(kknn)
library(tree)
library(rpart)
library(rstudioapi)
library(tree)
library(rJava)
library(partykit)
library(dplyr)
library(party)
library(caret)
library(ipred)
library(RWeka)
library(randomForest)
library(gbm)
cleantrain <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/cleantrain.csv")
View(cleantrain)
library(readr)
cleantest <- read_csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificación/DataMining/preprocess/cleantest.csv")
str(cleantest)
head(cleantest)
as.data.frame(cleantest)
library(readr)
cleantest <- read_csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificación/DataMining/preprocess/cleantest.csv",
col_names = FALSE)
View(cleantest)
str(cleantest)
as.data.frame(cleantest)
cleantrain <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/cleantrain.csv")
View(cleantrain)
getwd()
setwd("/Users/jaumecloquell/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificación/DataMining")
cleantrain <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/cleantrain.csv")
View(cleantrain)
ultimatraining <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/ultimatraining.csv")
View(ultimatraining)
library(randomForest)
library(NoiseFiltersR)
library(kknn)
library(tree)
library(rpart)
library(rstudioapi)
library(tree)
library(rJava)
library(partykit)
library(dplyr)
library(party)
library(caret)
library(ipred)
library(RWeka)
library(randomForest)
library(gbm)
# Definimos el path de donde estemos trabajando.
setwd(dirname(getActiveDocumentContext()$path))
# Librerias propias
for (file in list.files("preprocess")) {
source(paste("preprocess/", file, collapse = NULL, sep = ""))
}
out_univ <- function(i, train){
x = train[,i]
cuantiles <- quantile(x, c(0.25,0.75))
iqr = IQR(x)
x[x < cuantiles[1]-5*iqr | x > cuantiles[2]+5*iqr ] <- NA
return(x)
}
filtrar_univ <- function(train){
sapply(1:50, out_univ, train)
}
filtrar_IPC <- function(train){
filtrado <- IPF(C ~ ., train, s = 2)
return(as.data.frame(filtrado$cleanData))
}
outlier_imput <- function(i, test, train){
x <- train[,i]
y <- test[,i]
cuantiles <- quantile(x, c(0.25, 0.75))
iqr <- IQR(x)
y[y < cuantiles[1]-5*iqr | y > cuantiles[2]+5*iqr ] <- NA
formula <- paste("X", i, "~.", sep = "")
modelo <- kknn(formula, train, test)
y[is.na(y)] <- modelo$fitted.values[is.na(y)]
return(y)
}
FS_forest_importance <- function(formula, x, k=5, imp = 1){
weights <- FSelector::random.forest.importance(formula,x, importance.type = imp)
subset <- FSelector::cutoff.k(weights,k)
Y <- x[,51]
return(cbind(x[,subset], Y))
}
limpieza_total_test <- function(train, test, iter = 1){
for(i in 1:iter){
test <- as.data.frame(sapply(1:50, outlier_imput, test, train))
colnames(test) <- paste("X",1:50, sep = "")
}
return(test)
}
limpieza_total_train <- function(train, iter = 1){
train <- rfImpute(C ~ ., train, iter = 3)
for(i in 1:iter){
train[,2:51] <- filtrar_univ(train[,2:51])
train <- rfImpute(C ~., train, iter = 3)
}
train <- filtrar_IPC(train)
return(train)
}
original.dataset <- readData()
dataCleaned <- list()
train <- original.dataset$train
test <- original.dataset$test
train$C <- as.factor(train$C)
dataCleaned[["train"]] <- limpieza_total_train(train, 3)
FS_forest_importance("C~.", dataCleaned[["train"]])
str(dataCleaned[["train"]])
FS_forest_importance(C~., dataCleaned[["train"]])
# load the library
library(mlbench)
library(caret)
str(dataCleaned[["train"]])
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(dataCleaned[["train"]][,2:51], PimaIndiansDiabetes[,1], sizes=c(1:35), rfeControl=control)
# run the RFE algorithm
results <- rfe(dataCleaned[["train"]][,2:51], dataCleaned[["train"]][,1], sizes=c(1:35), rfeControl=control)
FS_forest_importance(C~., dataCleaned[["train"]], k = 10, imp = 3)
FS_forest_importance(C~., dataCleaned[["train"]], k = 10, imp = 3)
FS_forest_importance(C~., dataCleaned[["train"]], k = 10, imp = 1)
FS_forest_importance(C~., dataCleaned[["train"]], k = 40, imp = 1)
library(Boruta)
install.packages("Boruta")
library(Boruta)
set.seed(111)
boruta.bank_train <- Boruta(C~., data = dataCleaned[["train"]], doTrace = 2)
print(boruta.bank_train)
boruta.bank <- TentativeRoughFix(boruta.bank_train)
print(boruta.bank)
dataCleaned[["test"]] <- limpieza_total_test(dataCleaned[["train"]], test)
dataCleaned[["train"]]$C <- as.numeric(dataCleaned[["train"]]$C)
dataCleaned[["test"]] <- limpieza_total_test(dataCleaned[["train"]], test)
str(dataCleaned[["train"]])
str(test)
dataCleaned[["test"]] <- limpieza_total_test(dataCleaned[["train"]][2:51], test)
#Create tree model
trees <- tree(C~., dataCleaned[["train"]] )
plot(trees)
text(trees, pretty=0)
#Cross validate to see whether pruning the tree will improve performance
cv.trees <- cv.tree(trees)
plot(cv.trees)
prune.trees <- prune.tree(trees, best=4)
plot(prune.trees)
text(prune.trees, pretty=0)
yhat <- predict(prune.trees, dataCleaned[["test"]] )
plot(yhat, dataCleaned[["test"]]$C)
abline(0,1)
KaggleWiteData(1:dim(test)[1], yhat, "predictions")
# Definimos el path de donde estemos trabajando.
setwd(dirname(getActiveDocumentContext()$path))
writeData(dataCleaned[["train"]], path = "datosFiltrados/", "cleanTrainRfQuanIpc")
writeData(dataCleaned[["test"]], path = "datosFiltrados/", "cleanTestRfQuanIpc")
?predict
str(dataCleaned[["train"]])
dataCleaned[["train"]]$C <- as.factor(dataCleaned[["train"]]$C)
library(C50)
credit.fit <- C5.0(dataCleaned[["train"]], dataCleaned[["train"]]$C)
credit.fit
#Create tree model
trees <- tree(C~., dataCleaned[["train"]] )
plot(trees)
text(trees, pretty=0)
#Cross validate to see whether pruning the tree will improve performance
cv.trees <- cv.tree(trees)
plot(cv.trees)
prune.trees <- prune.tree(trees, best=4)
plot(prune.trees)
text(prune.trees, pretty=0)
yhat <- predict(prune.trees, dataCleaned[["test"]], type = "class")
plot(yhat, dataCleaned[["test"]]$C)
abline(0,1)
KaggleWiteData(1:dim(test)[1], yhat, "predictions")
prune.trees <- prune.tree(trees, best=10)
plot(prune.trees)
text(prune.trees, pretty=0)
summary(trees)$used
names(dataCleaned[["train"]])[which(!(names(dataCleaned[["train"]]) %in% summary(trees)$used))]
trees <- tree(C~-c("X1", "X32", "X17"), dataCleaned[["train"]] )
trees <- tree(C ~ -c("X1","X32", "X17"), dataCleaned[["train"]] )
trees <- tree(C ~ -c(X1,X32, X17), dataCleaned[["train"]] )
trees <- tree(C ~ . -X2 -X32 -X17, dataCleaned[["train"]] )
#Cross validate to see whether pruning the tree will improve performance
cv.trees <- cv.tree(trees)
plot(cv.trees)
prune.trees <- prune.tree(trees, best=10)
plot(prune.trees)
text(prune.trees, pretty=0)
yhat <- predict(prune.trees, dataCleaned[["test"]], type = "class")
plot(yhat, dataCleaned[["test"]]$C)
abline(0,1)
KaggleWiteData(1:dim(test)[1], yhat, "/predictions")
KaggleWiteData(1:dim(test)[1], yhat, "predictions/")
cleantrain <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/cleantrain.csv")
View(cleantrain)
library(tree)
library(rpart)
library(rstudioapi)
library(tree)
library(rJava)
library(partykit)
library(dplyr)
library(party)
library(caret)
library(ipred)
library(RWeka)
library(randomForest)
library(gbm)
#https://github.com/armijoalb/Master-Ciencias-de-Datos-UGR/blob/master/Miner%C3%ADa%20de%20datos.%20Preprocesamiento%20y%20clasificacion/practicas456/clasificacion/kaggle_arboles.Rmd
#https://github.com/armijoalb/Master-Ciencias-de-Datos-UGR/blob/master/Miner%C3%ADa%20de%20datos.%20Preprocesamiento%20y%20clasificacion/preprocesamiento.pdf
# Definimos el path de donde estemos trabajando.
setwd(dirname(getActiveDocumentContext()$path))
# Librerias propias
for (file in list.files("preprocess")) {
source(paste("preprocess/", file, collapse = NULL, sep = ""))
}
knitr::opts_chunk$set(echo = TRUE)
cleantrain <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/preprocess/cleantrain.csv")
View(cleantrain)
library(RWeka)
library(caret)
cleantest <- read.csv("~/Desktop/Master Ciencia de Datos/Mineria de datos Preprocesamiento y clasificaci\x03n/DataMining/decisionTree/datosFiltrados/cleantest.csv")
View(cleantest)
cleantrain <- read.csv("/datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
str(cleantrain)
cleantrain <- cleantrain[,2:dim(cleantrain)[2]]
str(cleantrain)
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- cleantrain[,2:dim(originalCleantrain)[2]]
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
summary(hr_base_model)
summary(hr_base_model)
#Plot Decision Tree
plot(hr_base_model)
# Examine the complexity plot
printcp(hr_base_model)
plotcp(hr_base_model)
test$pred <- predict(hr_base_model, cleantest, type = "class")
base_accuracy <- mean(test$pred == test$left)
base_accuracy
#Postpruning
# Prune the hr_base_model based on the optimal cp value
hr_model_pruned<- prune(hr_base_model, cp= hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"])
#hr_model_pruned <- prune(hr_base_model, cp = 0.0047 )
# Compute the accuracy of the pruned tree
test$pred <- predict(hr_model_pruned, test, type = "class")
accuracy_postprun <- mean(test$pred == test$left)
KaggleWiteData(1:dim(cleantest[,2:dim(cleantest)[2]])[1], test$pred, path = "predictions/")
str(cleantrain)
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
printcp(hr_base_model)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 2))
printcp(hr_base_model)
printcp(hr_base_model)
bestcp <- tree$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
printcp(hr_base_model)
bestcp <- tree$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 2))
printcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
fitControl <- trainControl(method = "cv", number = 2)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = fitControl)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
trControl = fitControl)
?rpart
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
printcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
summary(hr_base_model)
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
printcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
summary(hr_base_model)
#Plot Decision Tree
plot(hr_base_model)
# Examine the complexity plot
printcp(hr_base_model)
plotcp(hr_base_model)
test$pred <- predict(hr_base_model, cleantest, type = "class")
base_accuracy <- mean(test$pred == test$left)
base_accuracy
#Postpruning
# Prune the hr_base_model based on the optimal cp value
hr_model_pruned<- prune(hr_base_model, cp= hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"])
#hr_model_pruned <- prune(hr_base_model, cp = 0.0047 )
# Compute the accuracy of the pruned tree
test$pred <- predict(hr_model_pruned, test, type = "class")
accuracy_postprun <- mean(test$pred == test$left)
KaggleWiteData(1:dim(cleantest)[1], test$pred, path = "predictions/")
library(unbalanced)
install.packages("unbalanced")
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
library(unbalanced)
n<-ncol(cleantrain)
output<-cleantrain$C
input<-cleantrain[ ,-n]
data<-ubTomek(X = input, Y = output)
cleantrain<-cbind(data$X, data$Y)
str(cleantrain)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
cleantrain<-cbind(data$X, "C" = data$Y)
str(cleantrain)
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
printcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
hr_model_pruned<- prune(hr_base_model, cp= bestcp)
test$pred <- predict(hr_model_pruned, test, type = "class")
accuracy_postprun <- mean(test$pred == test$left)
KaggleWiteData(1:dim(cleantest)[1], test$pred, path = "predictions/")
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
library(unbalanced)
n<-ncol(cleantrain)
output<-cleantrain$C
input<-cleantrain[ ,-n]
data<-ubTomek(X = input, Y = output)
data<-cbind(data$X, "C" = data$Y)
data[,1:50] <- filtrar_univ(data[,1:50])
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
printcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
hr_model_pruned<- prune(hr_base_model, cp= bestcp)
test$pred <- predict(hr_model_pruned, test, type = "class")
accuracy_postprun <- mean(test$pred == test$left)
KaggleWiteData(1:dim(cleantest)[1], test$pred, path = "predictions/")
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
library(unbalanced)
n<-ncol(cleantrain)
output<-cleantrain$C
input<-cleantrain[ ,-n]
data<-ubTomek(X = input, Y = output)
data<-cbind(data$X, "C" = data$Y)
data[,1:50] <- filtrar_univ(data[,1:50])
cleantrain <- data
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
printcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
hr_model_pruned<- prune(hr_base_model, cp= bestcp)
test$pred <- predict(hr_model_pruned, test, type = "class")
accuracy_postprun <- mean(test$pred == test$left)
KaggleWiteData(1:dim(cleantest)[1], test$pred, path = "predictions/")
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
library(unbalanced)
n<-ncol(cleantrain)
output<-cleantrain$C
input<-cleantrain[ ,-n]
data<-ubTomek(X = input, Y = output)
data<-cbind(data$X, "C" = data$Y)
data <- limpieza_total_test(data, cleantest, 3)
str(data)
library(unbalanced)
n<-ncol(cleantrain)
output<-cleantrain$C
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
library(unbalanced)
n<-ncol(cleantrain)
output<-cleantrain$C
input<-cleantrain[ ,-n]
data<-ubTomek(X = input, Y = output)
data<-cbind(data$X, "C" = data$Y)
cleantrain <- data
cleantrain <- limpieza_total_train(cleantrain, 3)
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
library(unbalanced)
n<-ncol(cleantrain)
output<-cleantrain$C
input<-cleantrain[ ,-n]
data<-ubTomek(X = input, Y = output)
data<-cbind(data$X, "C" = data$Y)
cleantrain <- limpieza_total_train(cleantrain, 3)
?rfImpute.default
originalCleantrain <- read.csv("datosFiltrados/cleantrain.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
originalCleantest <- read.csv("datosFiltrados/cleantest.csv", header = T, sep = ",", na.strings = c("?","NA",".",""))
cleantrain <- originalCleantrain[,2:dim(originalCleantrain)[2]]
cleantest <- originalCleantest[,2:dim(originalCleantest)[2]]
cleantrain$C <- as.factor(cleantrain$C)
library(unbalanced)
n<-ncol(cleantrain)
output<-cleantrain$C
input<-cleantrain[ ,-n]
data<-ubTomek(X = input, Y = output)
data<-cbind(data$X, "C" = data$Y)
data[,1:50] <- filtrar_univ(data[,1:50])
data <- rfImpute(C ~., data, iter = 3)
cleantrain <- data
hr_base_model <- rpart(C ~ ., data = cleantrain, method = "class",
control = rpart.control(cp = 0))
printcp(hr_base_model)
bestcp <- hr_base_model$cptable[which.min(hr_base_model$cptable[,"xerror"]),"CP"]
bestcp
hr_model_pruned<- prune(hr_base_model, cp= bestcp)
test$pred <- predict(hr_model_pruned, test, type = "class")
accuracy_postprun <- mean(test$pred == test$left)
KaggleWiteData(1:dim(cleantest)[1], test$pred, path = "predictions/")
?ubBalance
